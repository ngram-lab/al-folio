---
layout: page
permalink: /seminar/
title: seminar
order: 6.5
description:
nav: true

---

# UMass NLP Seminar

This webpage will contain information on the Fall 2025 NLP seminar.
See further below for information about the seminar as a course.

## Schedule: Fall 2025

The NLP seminar takes place Wednesdays, 12:30pm - 1:45pm, in room LGRC A104 and on Zoom.

- Sept. 10: organizational meeting
- Sept. 17: [Katrin Erk](https://www.katrinerk.com/), UMass CICS and Linguistics, "Analyzing word token embeddings to assess meaning in context."  (In-person talk.)
  - *Abstract:* Word token embeddings constitute a condensed record of utterances of many speakers. This makes them interesting as data for lexical semantics, for questions like: Can word embeddings tell us about the structure of polysemous words, and which properties distinguish different usage groups? Do word embeddings encode meaning distinctions that differ from dictionary senses, and if so, is this data or noise? Can word embeddings help us assess the subtle meaning changes that  constructions impose on their components? And, importantly: How can we test the performance of methods for analyzing word token embeddings, and under what circumstances can we rely on them? In this talk I discuss methods that we have been developing for studying lexical semantics through embeddings, and analyses we've done so far. 
  - *Bio:* Katrin Erk is a professor in the CS and Linguistics department at UMass Amherst. Her research expertise is in the area of computational linguistics, especially semantics. Katrin Erk completed her dissertation on tree description languages and ellipsis in the CS department at Saarland University in 2002. She did a postdoc in the computational linguistics department at Saarland University from 2002 to 2006. In 2006, she joined the Linguistics Department at the University of Texas at Austin, and starting in 2023 she had a joint appointment between the Linguistics and CS departments. She joined UMass Amherst in 2025. 
  - *Related papers:*
  [Chronis et al., ACL 2023](https://aclanthology.org/2023.acl-long.14/) and [Erk and Apidianaki, NAACL 2024](https://aclanthology.org/2024.naacl-long.146/).
- Sept. 24: [Catherine Arnett](https://www.catherinearnett.github.io/), EleutherAI, "Why do language models perform worse for morphologically complex languages?"
  - *Abstract:* Language models perform differently across languages. It has been previously suggested that morphological typology may explain some of this variability (Cotterell et al., 2018). We replicate previous analyses and find additional new evidence for a performance gap between agglutinative and fusional languages, where fusional languages, such as English, tend to have better language modeling performance than morphologically more complex languages like Turkish. We then propose and test three possible causes for this performance gap: morphological alignment of tokenizers, tokenization quality, and disparities in dataset sizes and measurement. We find some evidence that tokenization quality explains the performance gap, but none for the role of morphological alignment. Instead we find that the performance gap is most reduced when training datasets are of equivalent size across language types, but only when scaled according to the so-called "byte-premium"---the different encoding efficiencies of different languages and orthographies. These results suggest that languages of particular morphological types are not intrinsically advantaged or disadvantaged in language modeling. These findings bear on ongoing efforts to improve performance for low-performing and under-resourced languages.
  - *Bio:* Catherine Arnett is an NLP Researcher at EleutherAI. She received her PhD in Linguistics with a specialization in Computational Social Science from the University of California San Diego.She is primarily interested in cross-lingual and multilingual NLP and has worked on related topics, such as cross-lingual transfer, tokenization, and dataset creation. Her work aims to use linguistically informed methods to study how cross-linguistic differences interact with language models and to improve language technologies for non-English languages.
  - *Related papers:* [Why do language models perform worse for morphologically complex languages?](https://aclanthology.org/2025.coling-main.441/) (Arnett & Bergen, 2025), [A bit of a problem: Measurement disparities in dataset sizes across languages](https://aclanthology.org/2024.sigul-1.1.pdf) (Arnett et al., 2024), and [Goldfish: Monolingual language models for 350 languages](https://arxiv.org/pdf/2408.10441).
- Oct. 1: Paper discussion: [Baumann et al., 2025 (via arxiv)](https://arxiv.org/abs/2509.08825). Large language model hacking: Quantifying the hidden risks of using LLMs for text annotation.
- Oct. 8: No session
- Oct. 15: [Alisa Liu](https://alisawuffles.github.io/), University of Washington
- Oct. 22: [Emma Pierson](https://people.eecs.berkeley.edu/~emmapierson/), University of California, Berkeley
- Oct. 29: [Os Keyes](https://ironholds.org/), University of Massachusetts Lowell 
- Nov. 5: [Jack Wang](https://zichaow.github.io/), Adobe Research
- Nov. 12: TBA
- Nov. 19: [Shira Wein](https://shirawein.github.io/), Amherst College.  (In-person talk.)
- Nov. 26: No seminar (Thanksgiving break)
- Dec. 3: TBA

## Course: COMPSCI 692L

The seminar is available as a 1-credit seminar course, 692L.
Enrollment in the course is not required to attend talks.

Course requirements: Students must read posted papers before the class sessions, submit questions before the seminar, and be prepared to ask their questions (or other questions) at the speaker's talk.  In class sessions without a speaker, students should be ready to participate in class discussions.

Assignment details are distributed through the seminar's slack channel.

## Previous seminars

Speakers from some previous offerings of the NLP Seminar:
  <a href="/seminar_s25/">Spring 2025</a>,
  <a href="https://people.cs.umass.edu/~miyyer/nlpseminar/">Fall 2024</a>,
  <a href="https://people.cs.umass.edu/~miyyer/nlpseminar/spring24.html">Spring 2024</a>,
  <a href="https://people.cs.umass.edu/~miyyer/nlpseminar/fall23.html">Fall 2023</a>,
  <a href="https://people.cs.umass.edu/~miyyer/nlpseminar/spring23.html">Spring 2023</a>.

